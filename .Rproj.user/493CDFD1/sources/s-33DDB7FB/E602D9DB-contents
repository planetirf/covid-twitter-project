---
title: "covid_eda"
author: "Irfan Ainuddin, Ashley Person, Chicago"
date: "4/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(textdata)
library(scales)
library(gridExtra)
```

```{r}
## import data and clean
## deal with scientific notation
options(scipen = 999)

df <- readRDS(file = "covid-tweets-2020-04-17.rds")

who <- readRDS(file = "who_tweets.Rds")
cdc <- readRDS(file = "cdc_tweets.Rds")
```


```{r}
remove_reg <- "&amp;|&lt;|&gt;"
```

```{r}
## create working tweet tibble
tweets <- tibble(tweet_id=df$tweet_id, text=df$text)

## Preparing public twitter text for sentiment analysis 
tweet_words <- tweets %>% 
                  ## remove the words from remove_reg variable above
                  mutate(text = str_remove_all(text, remove_reg)) %>%
                  ## tokenize the words for sentiment analysis
                  unnest_tokens(word, text, token="tweets", strip_url = TRUE) %>%
                  ## filter out stop words using all libraries(SMART, onix, snowball)
                  filter(!word %in% stop_words$word,
                         !word %in% str_remove_all(stop_words$word, "'"))

## tweets broken into each word.
tweet_words
```

```{r}
## create working tweet tibble
cdc_tweets <- tibble(tweet_id=cdc$status_id, text=cdc$text)

## Preparing public twitter text for sentiment analysis 
cdc_tweet_words <- cdc_tweets %>% 
                  ## remove the words from remove_reg variable above
                  mutate(text = str_remove_all(text, remove_reg)) %>%
                  ## tokenize the words for sentiment analysis
                  unnest_tokens(word, text, token="tweets", strip_url = TRUE) %>%
                  ## filter out stop words using all libraries(SMART, onix, snowball)
                  filter(!word %in% stop_words$word,
                         !word %in% str_remove_all(stop_words$word, "'"))

## tweets broken into each word.
cdc_tweet_words
```


```{r}
who_tweets <- tibble(tweet_id=who$status_id, text=who$text)

## Preparing public twitter text for sentiment analysis 
who_tweet_words <- who_tweets %>% 
                  ## remove the words from remove_reg variable above
                  mutate(text = str_remove_all(text, remove_reg)) %>%
                  ## tokenize the words for sentiment analysis
                  unnest_tokens(word, text, token="tweets", strip_url = TRUE) %>%
                  ## filter out stop words using all libraries(SMART, onix, snowball)
                  filter(!word %in% stop_words$word,
                         !word %in% str_remove_all(stop_words$word, "'"))

## tweets broken into each word.
who_tweet_words
```

```{r}
## apply NRC sentiments to ALL
tweet_words_nrc <- tweet_words %>% 
          inner_join(get_sentiments("nrc"))
## apply NRC sentiments CDC
cdc_tweet_words_nrc <- cdc_tweet_words %>% 
          inner_join(get_sentiments("nrc"))
## apply NRC sentiments WHO
who_tweet_words_nrc <- who_tweet_words %>% 
          inner_join(get_sentiments("nrc"))
```


```{r}
## Add counts for each sentiment w/ pivot_wider()
tweet_count <- tweet_words_nrc %>% group_by(tweet_id, sentiment) %>% count() %>%   pivot_wider(names_from = sentiment, values_from= n)

## Add counts foreach sentiment w/ pivot_wider()
cdc_tweet_count <- cdc_tweet_words_nrc %>% group_by(tweet_id, sentiment) %>% count() %>% pivot_wider(names_from = sentiment, values_from= n)

## Add counts for each sentiment w/ pivot_wider()
who_tweet_count <- who_tweet_words_nrc %>% group_by(tweet_id, sentiment) %>% count() %>% pivot_wider(names_from = sentiment, values_from= n)

```


```{r}
## Sentiment Counts
all_sentiment_count <- colSums(tweet_count[,-1], na.rm = TRUE)

## CDC sentiment count
cdc_sentiment_count <- colSums(cdc_tweet_count[,-1], na.rm = TRUE)

# WHO sentiment count
who_sentiment_count <- colSums(who_tweet_count[,-1], na.rm = TRUE)

## convert from named list to data frame for all
all_sentiment_sum = data.frame(count=all_sentiment_count, sentiment=names(all_sentiment_count))
cdc_sentiment_sum = data.frame(count=cdc_sentiment_count, sentiment=names(cdc_sentiment_count))
who_sentiment_sum = data.frame(count=who_sentiment_count, sentiment=names(who_sentiment_count))
```

```{r}
## set factor levels for all
all_sentiment_sum$sentiment = factor(all_sentiment_sum$sentiment, levels=all_sentiment_sum$sentiment[order(all_sentiment_sum$count, decreasing = TRUE)])
## set factor levels for CDC
cdc_sentiment_sum$sentiment = factor(cdc_sentiment_sum$sentiment, levels=cdc_sentiment_sum$sentiment[order(cdc_sentiment_sum$count, decreasing = TRUE)])
## set factor levels for WHO
who_sentiment_sum$sentiment = factor(who_sentiment_sum$sentiment, levels=who_sentiment_sum$sentiment[order(who_sentiment_sum$count, decreasing = TRUE)])

```

```{r}

gg_count_all <- all_sentiment_sum %>% ggplot(aes(reorder(sentiment, -count), count)) + 
  geom_bar(stat="identity", aes(fill=sentiment)) +
  geom_text(aes(label=count)) +
  theme(legend.position = "none") +
  xlab('Covid Tweet Archive Sentiments')

gg_count_cdc <- cdc_sentiment_sum %>% ggplot(aes(reorder(sentiment, -count), count)) + 
  geom_bar(stat="identity", aes(fill=sentiment)) +
  geom_text(aes(label=count)) +
  theme(legend.position = "none") +
  xlab('CDC Tweet Sentiments') + 
  ylim(0,50000)



gg_count_who <- who_sentiment_sum %>% ggplot(aes(reorder(sentiment, -count), count)) + 
  geom_bar(stat="identity", aes(fill=sentiment)) +
  geom_text(aes(label=count)) +
  theme(legend.position = "none") +
  xlab('WHO Tweet Sentiments') + 
  ylim(0,50000)

grid.arrange(gg_count_all,gg_count_cdc,gg_count_who, ncol=1)
gg_count_all
gg_count_cdc
gg_count_who
```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
